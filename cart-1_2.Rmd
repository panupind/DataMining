---
  title: "CART"
output:
  html_document: default
pdf_document: default
---
  
```{r include=FALSE}
#Replace the below command to point to your working directory
#setwd ("D:/Academic Operations/Data Mining/Supervised Machine Learning Techniques")
getwd()

#Lets begin by loading a very simple dataset that has two independent variables and a dependent variable that is simply labeled "Good" or "Bad". This simple dataset will make visualization easier
remove(trainDS)
trainDS <- read.csv(file ="D:\\Prasanna\\Personal\\Learning\\Other Reads\\GL\\Course Material\\Data Mining\\Practice Material & Questions Papers\\Solving Practice Problems\\Practice Quiz\\data.csv" , header = TRUE)
print(trainDS)
str(trainDS)
```

``` {r}
#loading the data set into the variable trainDS
#trainDS = read.csv("(CART)simpleDTDS.csv", sep = ",", header = TRUE)
#looking at the variable trainDS
trainDS
#removing the first column of trainDS
#trainDS = trainDS[,-1]
#gives first n rows of trainDS
head(trainDS)
#gives number of rows of trainDS
nrow(trainDS)
#gives a count of the different entries in the Y column in trainDS

#gives the fraction of "Good" entries in the Y column in trainDS
sum(trainDS$delinquent=="Yes")/nrow(trainDS)

#rows(trainDS$delinquent[trainDS$delinquent=="Yes"])
```


Now lets plot it
```{r}
#plotting X1 and X2 of trainDS in a 2-D frame
#plot(trainDS$delinquent,trainDS$term)
#colouring all the "Good" points under variable Y of trainDS blue in colour
#points(trainDS$X1[trainDS$Y=="Good"],trainDS$X2[trainDS$Y=="Good"],col="blue",pch=19)
#colouring all the "Bad" points under variable Y of trainDS red in colour
#points(trainDS$X1[trainDS$Y=="Bad"],trainDS$X2[trainDS$Y=="Bad"],col="red",pch=19)
```


#We will use the "rpart" and the "rpart.plot" libraries to build decision trees. We begin by building a very complex classification tree, by setting the "cost complexity" threshold to "0" and the minimum bucket size to be 3. Then we plot the tree using rpart.#
```{r}
#installs the library rpart if it is not installed in the system
#install.packages('rpart')
#installs the library rpart.plot if it is not installed in the system
#install.packages('rpart.plot')
#loads the library rpart into the system
trainDS$term = as.factor(trainDS$term)
trainDS$gender=as.factor(trainDS$gender)
trainDS$age=as.factor(trainDS$age)
trainDS$FICO = as.factor(trainDS$FICO)
names(trainDS)
trainDS
str(trainDS)
```

```{r}
names(trainDS)
```


```{r}
library(rpart)
#loads the library rpart.plot into the system
library(rpart.plot)

#here we have built a formula where the variable Y is dependent on all the variables of trainDS
trainDS <- trainDS[,-1]
tree <- rpart(formula = trainDS$Sdelinquent~., 
#building a decision tree
              data = trainDS[,-1], method = "class" )
#printing the decision tree in the console
tree
#plots the tree in a graphical manner
rpart.plot(tree )
```


The cost complexity table can be obtained using the printcp or plotcp functions
```{r}
printcp(tree)
plotcp(tree)
```

The unncessarily complex tree above can be pruned using a cost complexity threshold. Using a complexity threshold of 0.015 gives us a much simpler tree.
```{r}
ptree = prune(tree, cp= 0.015 ,"CP")
printcp(ptree)
ptree
rpart.plot(ptree)
```

Next for a given decision tree, we can tease out the business rules for each end node using rpart.path
```{r}
#gives the path by which we are going to arrive at that particualr node
path.rpart(ptree,c(4,5,6,7))
```

Since we only have two independent variables, we can visualize the decision tree in a different and more convenient way.
```{r}
plot(trainDS$X1,trainDS$X2)
points(trainDS$X1[trainDS$Y=="Good"],trainDS$X2[trainDS$Y=="Good"],col="blue",pch=19)
points(trainDS$X1[trainDS$Y=="Bad"],trainDS$X2[trainDS$Y=="Bad"],col="red",pch=19)

#split 1 at X2=0.4
lines(c(0,1),c(.4,.4))
#split 2 at X1=0.75
lines(c(0.75,0.75),c(0,.4))
#split 3 at X1=0.5
lines(c(0.5,0.5),c(.4,1))

#this kind of partition is in line with the decision tree plotted above

trainDS
```

Finally, we might want to use the decision tree to predict the class for each row and/or score the probabilities:


```{r}
predict(tree, data=trainDSNew, type="class")
```


```{r}
remove(trainDSNew)
trainDSNew <- trainDS[,-1]
trainDSNew
str(trainDSNew)
#as per our model, we are finding  the prediction of our Y variable
trainDSNew$prediction = predict(tree, data=trainDSNew, type="class")
#as per the model built by us, we are finding the probablity of the Y variable being either "Good" or "Bad"
trainDSNew$prediction 
trainDSNew$score = predict(tree, data=trainDSNew, type="prob" , list = FALSE)
trainDSNew$score
view(trainDSNew)
```
```{r}
tabtest=with(trainDSNew,table(trainDSNew$Sdelinquent,trainDSNew$prediction))
tabtest
(2844  + 983 +738 + 6983)-nrow(trainDS)
classificationError <- (738 + 983 ) / (2844  + 983 +738 + 6983)
classificationError
```

```{r}
library(pROC)

roc_obj = roc(trainDSNew$Sdelinquent, trainDSNew$score[,2])


plot(roc_obj, print.auc = T)
```

